{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Support functions\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Scoring functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing our clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cleanData.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "df=clean_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model fitting, we will try out the following :\n",
    "\n",
    "* Logistic Regression\n",
    "* KNeighbors\n",
    "* SVC\n",
    "* Naive Bayes\n",
    "* Decision tree\n",
    "* Random Forest\n",
    "* SDG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.washDishes.values\n",
    "x = df.drop(['washDishes'], axis = 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "accuracies = {}\n",
    "lr = LogisticRegression(max_iter=250) \n",
    "lr.fit(x_train,y_train)\n",
    "acc = lr.score(x_test,y_test)*100\n",
    "accuracies['Logistic Regression'] = acc\n",
    "print(\"Test Accuracy {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "print(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try ro find best k value\n",
    "scoreList = []\n",
    "for i in range(1,20):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
    "    knn2.fit(x_train, y_train)\n",
    "    scoreList.append(knn2.score(x_test, y_test))\n",
    "    \n",
    "plt.plot(range(1,20), scoreList)\n",
    "plt.xticks(np.arange(1,20,1))\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "acc = max(scoreList)*100\n",
    "accuracies['KNN'] = acc\n",
    "print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state = 1)\n",
    "svm.fit(x_train, y_train)\n",
    "\n",
    "acc = svm.score(x_test,y_test)*100\n",
    "accuracies['SVM'] = acc\n",
    "print(\"Maximum SVM Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "acc = nb.score(x_test,y_test)*100\n",
    "accuracies['Naive Bayes'] = acc\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(x_train, y_train)\n",
    "\n",
    "acc = dtc.score(x_test, y_test)*100\n",
    "accuracies['Decision Tree'] = acc\n",
    "print(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "acc = rf.score(x_test,y_test)*100\n",
    "accuracies['Random Forest'] = acc\n",
    "print(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(max_iter=250)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "acc = clf.score(x_test,y_test)*100\n",
    "accuracies['SDG Classifier'] = acc\n",
    "print(\"SDG Classifier Algorithm Accuracy Score : {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values\n",
    "y_head_lr = lr.predict(x_test)\n",
    "knn19 = KNeighborsClassifier(n_neighbors = 19)\n",
    "knn19.fit(x_train, y_train)\n",
    "y_head_knn = knn19.predict(x_test)\n",
    "y_head_svm = svm.predict(x_test)\n",
    "y_head_nb = nb.predict(x_test)\n",
    "y_head_dtc = dtc.predict(x_test)\n",
    "y_head_rf = rf.predict(x_test)\n",
    "y_head_clf= clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_lr = confusion_matrix(y_test,y_head_lr)\n",
    "cm_knn = confusion_matrix(y_test,y_head_knn)\n",
    "cm_svm = confusion_matrix(y_test,y_head_svm)\n",
    "cm_nb = confusion_matrix(y_test,y_head_nb)\n",
    "cm_dtc = confusion_matrix(y_test,y_head_dtc)\n",
    "cm_rf = confusion_matrix(y_test,y_head_rf)\n",
    "cm_clf=confusion_matrix(y_test,y_head_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\",fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "sns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"K Nearest Neighbors Confusion Matrix\")\n",
    "sns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.title(\"Support Vector Machine Confusion Matrix\")\n",
    "sns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "sns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Decision Tree Classifier Confusion Matrix\")\n",
    "sns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "sns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python383jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}