{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07a63f3d33346feb89396e44d2106d71a898db4d716a84ec7a90cd4a8106e1c30",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gradient Boosting Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cleanData.ipynb\n",
    "%run cleanData_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data into float to be used in our algorithm\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "df_train=clean_dataset(df_train)\n",
    "df_test =clean_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using 80-20 split for train-test\n",
    "VALID_SIZE = 0.2\n",
    "#We also use random state for reproducibility\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "train, valid = train_test_split(df_train, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )"
   ]
  },
  {
   "source": [
    "We're going to split the data between predictors and target"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age',\n",
    "'gender',\n",
    "'scentLover',\n",
    "'ecoInterest',\n",
    "'MrPropre',\n",
    "'Antikal',\n",
    "'Ariel',\n",
    "'Dash',\n",
    "'pods',\n",
    "'powder',\n",
    "'liquid',\n",
    "'electricToothbrush',\n",
    "'likesPets',\n",
    "'hasPet',\n",
    "'daysSinceActivity',\n",
    "'nbChildren',\n",
    "'magasin',\n",
    "'moyenneSurface',\n",
    "'superMarket',\n",
    "'hyperMarket',\n",
    "'drive',\n",
    "'hardDiscount']\n",
    "target = 'washDishes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[predictors]\n",
    "train_Y = train[target].values\n",
    "valid_X = valid[predictors]\n",
    "valid_Y = valid[target].values"
   ]
  },
  {
   "source": [
    "We're going to test the algorithm with different learning rates:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_estimators = 200 #number of boosting stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "no_estimators_list = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "bestSore = 0\n",
    "bestParams = (0,0)\n",
    "\n",
    "for lr in lr_list:\n",
    "    for n in no_estimators_list:\n",
    "        clf = GradientBoostingClassifier(n_estimators = n, learning_rate = lr)\n",
    "        clf.fit(train_X, train_Y)\n",
    "\n",
    "    print(\"Learning rate:\", lr)\n",
    "    print(\"Accuracy (training): {0:.3f}\".format(clf.score(train_X, train_Y)))\n",
    "    print(\"Accuracy (validation): {0:.3f}\".format(clf.score(valid_X, valid_Y)))\n",
    "\n",
    "    if clf.score(valid_X, valid_Y) > bestScore:\n",
    "        bestScore = clf.score(valid_X, valid_Y)\n",
    "        bestParams = (lr, n)\n",
    "\n",
    "print(\"The best score (validation accuracy) is:\", bestScore)\n",
    "print(\"Parameters (learning rate, nb of estimators):\", bestParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\TekAdvice\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass loss={'n_estimators': (100, 200, 300, 500, 600), 'learning_rate': (0.05, 0.075, 0.1, 0.25, 0.5, 0.7, 0.8, 0.85, 0.9, 1)} as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported format string passed to dict.__format__",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-27a225f7ee1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best scores:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_check_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m         if (self.loss not in self._SUPPORTED_LOSS\n\u001b[0;32m    235\u001b[0m                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss '{0:s}' not supported. \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'deviance'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to dict.__format__"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': (100, 200, 300, 500, 600),\n",
    "    'learning_rate': (0.05, 0.075, 0.1, 0.25, 0.5, 0.7,0.8, 0.85, 0.9, 1)\n",
    "}\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(parameters)\n",
    "clf = clf.fit(train_X, train_Y)\n",
    "\n",
    "print('Best scores:',clf.best_score_)\n",
    "print('Best params:',clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}