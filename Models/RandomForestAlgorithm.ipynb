{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd04e3844b72a7cf8ab0d3d9d1f66ba811b683ddb4d84051a42926a17d4fe42a429",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import of the clean data from the train_dataset and the test_dataset\n",
    "%run cleanData.ipynb\n",
    "%run cleanData_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data into float to be used in our algorithm\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "df_train=clean_dataset(df_train)\n",
    "df_test =clean_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using 80-20 split for train-test\n",
    "VALID_SIZE = 0.2\n",
    "#We also use random state for reproducibility\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "train, valid = train_test_split(df_train, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "source": [
    "First, we define the predictors variables, then the target variable to predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the predictor variables and the target variable\n",
    "predictors = ['age',\n",
    "'gender',\n",
    "'scentLover',\n",
    "'ecoInterest',\n",
    "'MrPropre',\n",
    "'Antikal',\n",
    "'Ariel',\n",
    "'Dash',\n",
    "'pods',\n",
    "'powder',\n",
    "'liquid',\n",
    "'electricToothbrush',\n",
    "'likesPets',\n",
    "'hasPet',\n",
    "'daysSinceActivity',\n",
    "'nbChildren',\n",
    "'magasin',\n",
    "'moyenneSurface',\n",
    "'superMarket',\n",
    "'hyperMarket',\n",
    "'drive',\n",
    "'hardDiscount',\n",
    "'interested_by_fairypeps_email',\n",
    "'level_of_interest_about_marketing',\n",
    "'number_of_actions'\n",
    "]\n",
    "target = 'washDishes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split the dataset into a train and a validation sets\n",
    "train_X = train[predictors]\n",
    "train_Y = train[target].values\n",
    "valid_X = valid[predictors]\n",
    "valid_Y = valid[target].values\n"
   ]
  },
  {
   "source": [
    "Implementation of RandomForest Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\n",
    "NUM_ESTIMATORS = 2000 #number of estimators used for RandomForrestClassifier\n",
    "NO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of the RandomForest classifier\n",
    "clf = RandomForestClassifier(n_jobs=NO_JOBS, \n",
    "                             random_state=RANDOM_STATE,\n",
    "                             criterion=RFC_METRIC,\n",
    "                             n_estimators=NUM_ESTIMATORS,\n",
    "                             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of the model\n",
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation of the model on the remaining 20% of the training set\n",
    "preds = clf.predict(valid_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to better understand the importance of each variable, we want to plot the features importance.\n",
    "def plot_feature_importance():\n",
    "    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\n",
    "    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "    plt.figure(figsize = (7,4))\n",
    "    plt.title('Features importance',fontsize=14)\n",
    "    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance()"
   ]
  },
  {
   "source": [
    "'daysSinceActivity' and 'Age' are the two most important features in the prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Evaluation of our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "clf.score(train_X, train_Y)\n",
    "acc = round(clf.score(train_X, train_Y) * 100, 2)\n",
    "print(\"RandomForest accuracy (train set):\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf.score(valid_X, valid_Y)\n",
    "acc = round(clf.score(valid_X, valid_Y) * 100, 2)\n",
    "print(\"RandomForest accuracy (validation set):\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(valid_Y, preds, target_names=['Hand', 'Auto']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix():\n",
    "    cm = pd.crosstab(valid_Y, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "    fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "    sns.heatmap(cm, \n",
    "                xticklabels=['Hand', 'Auto'],\n",
    "                yticklabels=['Hand', 'Auto'],\n",
    "                annot=True,ax=ax1,\n",
    "                linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix()"
   ]
  },
  {
   "source": [
    "Let's initialize the GradientSearchCV parameters for optimization. We will set only few parameters, as following:\n",
    "\n",
    "n_estimators: number of trees in the foreset;\n",
    "\n",
    "max_features: max number of features considered for splitting a node;\n",
    "\n",
    "max_depth: max number of levels in each decision tree;\n",
    "\n",
    "min_samples_split: min number of data points placed in a node before the node is split;\n",
    "\n",
    "min_samples_leaf: min number of data points allowed in a leaf node."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = clf.fit(train_X, train_Y) #Gradient Search\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': (50, 100, 200),\n",
    "    'max_features': ('auto', 'sqrt'),\n",
    "    'max_depth': (3,4,5),\n",
    "    'min_samples_split': (2,5,10),\n",
    "    'min_samples_leaf': (1,2,3)\n",
    "}\n",
    "\n",
    "#We initialize GridSearchCV with the classifier, the set of parameters, number of folds and also the level of verbose for printing out progress.\n",
    "\n",
    "gs_clf = GridSearchCV(rf_clf, parameters, n_jobs=-1, cv = 5, verbose = 5)\n",
    "gs_clf = gs_clf.fit(train_X, train_Y)\n",
    "\n",
    "print('Best scores:',gs_clf.best_score_)\n",
    "print('Best params:',gs_clf.best_params_)\n",
    "\n",
    "#Let's predict with the validation data.\n",
    "\n",
    "preds = gs_clf.predict(valid_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.score(valid_X, valid_Y)\n",
    "acc = round(gs_clf.score(valid_X, valid_Y) * 100, 2)\n",
    "print(\"RandomForest accuracy optimized (validation set):\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(valid_Y, preds, target_names=['Hand', 'Auto']))"
   ]
  },
  {
   "source": [
    "Use of our model in the test dataset to submit on Kaggle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = gs_clf.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we have drop the userId column, we have to add it again next to the predicted values for the column \"washDishes\" so we get the userId again from the data set test.\n",
    "df_test_full = pd.read_csv(r\"DS_CentraleSupelec_ST42021/DS_CentraleSupelec_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prediction = pd.DataFrame(prediction_test, columns =['WashDishes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prediction['WashDishes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([df_test_full['userId'],result_prediction['WashDishes']], axis=1)\n",
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting the submit data to fit the submission format expected by Kaggle.\n",
    "submit.rename(columns={\"washDishes\": \"WashDishes\"})\n",
    "\n",
    "submit['WashDishes'] = submit['WashDishes'].apply(lambda e: 'Auto' if e == 1 else 'Hand')\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./DS_CentraleSupelec_ST42021/submit_final.csv', index=False)  "
   ]
  },
  {
   "source": [
    "In this second part, we will optimize the hyperparameters choices.\n",
    "We are going to use the Gradient Search for that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}