{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07a63f3d33346feb89396e44d2106d71a898db4d716a84ec7a90cd4a8106e1c30",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gradient Boosting Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting xgboost\n  Downloading xgboost-1.4.2-py3-none-win_amd64.whl (97.8 MB)\nRequirement already satisfied: numpy in c:\\users\\tekadvice\\anaconda3\\lib\\site-packages (from xgboost) (1.18.5)\nRequirement already satisfied: scipy in c:\\users\\tekadvice\\anaconda3\\lib\\site-packages (from xgboost) (1.5.0)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-1.4.2\n"
     ]
    }
   ],
   "source": [
    "%run cleanData.ipynb\n",
    "%run cleanData_test.ipynb\n",
    "\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data into float to be used in our algorithm\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "df_train=clean_dataset(df_train)\n",
    "df_test =clean_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using 80-20 split for train-test\n",
    "VALID_SIZE = 0.2\n",
    "#We also use random state for reproducibility\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "train, valid = train_test_split(df_train, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )"
   ]
  },
  {
   "source": [
    "We're going to split the data between predictors and target"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age',\n",
    "'gender',\n",
    "'scentLover',\n",
    "'ecoInterest',\n",
    "'MrPropre',\n",
    "'Antikal',\n",
    "'Ariel',\n",
    "'Dash',\n",
    "'pods',\n",
    "'powder',\n",
    "'liquid',\n",
    "'electricToothbrush',\n",
    "'likesPets',\n",
    "'hasPet',\n",
    "'daysSinceActivity',\n",
    "'nbChildren',\n",
    "'magasin',\n",
    "'moyenneSurface',\n",
    "'superMarket',\n",
    "'hyperMarket',\n",
    "'drive',\n",
    "'hardDiscount']\n",
    "target = 'washDishes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[predictors]\n",
    "train_Y = train[target].values\n",
    "valid_X = valid[predictors]\n",
    "valid_Y = valid[target].values"
   ]
  },
  {
   "source": [
    "We're going to test the algorithm with different learning rates:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 0.1\n",
      "Number of estimators: 100\n",
      "Accuracy (training): 0.661\n",
      "Accuracy (validation): 0.610\n",
      "Learning rate: 0.1\n",
      "Number of estimators: 200\n",
      "Accuracy (training): 0.674\n",
      "Accuracy (validation): 0.609\n",
      "Learning rate: 0.1\n",
      "Number of estimators: 300\n",
      "Accuracy (training): 0.684\n",
      "Accuracy (validation): 0.610\n",
      "Learning rate: 0.1\n",
      "Number of estimators: 400\n",
      "Accuracy (training): 0.700\n",
      "Accuracy (validation): 0.601\n",
      "Learning rate: 0.1\n",
      "Number of estimators: 500\n",
      "Accuracy (training): 0.705\n",
      "Accuracy (validation): 0.598\n",
      "Learning rate: 0.1\n",
      "Number of estimators: 600\n",
      "Accuracy (training): 0.715\n",
      "Accuracy (validation): 0.599\n",
      "Learning rate: 0.25\n",
      "Number of estimators: 100\n",
      "Accuracy (training): 0.680\n",
      "Accuracy (validation): 0.608\n",
      "Learning rate: 0.25\n",
      "Number of estimators: 200\n",
      "Accuracy (training): 0.706\n",
      "Accuracy (validation): 0.605\n",
      "Learning rate: 0.25\n",
      "Number of estimators: 300\n",
      "Accuracy (training): 0.730\n",
      "Accuracy (validation): 0.604\n",
      "Learning rate: 0.25\n",
      "Number of estimators: 400\n",
      "Accuracy (training): 0.747\n",
      "Accuracy (validation): 0.598\n",
      "Learning rate: 0.25\n",
      "Number of estimators: 500\n",
      "Accuracy (training): 0.760\n",
      "Accuracy (validation): 0.597\n",
      "Learning rate: 0.25\n",
      "Number of estimators: 600\n",
      "Accuracy (training): 0.773\n",
      "Accuracy (validation): 0.596\n",
      "Learning rate: 0.5\n",
      "Number of estimators: 100\n",
      "Accuracy (training): 0.705\n",
      "Accuracy (validation): 0.602\n",
      "Learning rate: 0.5\n",
      "Number of estimators: 200\n",
      "Accuracy (training): 0.739\n",
      "Accuracy (validation): 0.600\n",
      "Learning rate: 0.5\n",
      "Number of estimators: 300\n",
      "Accuracy (training): 0.763\n",
      "Accuracy (validation): 0.599\n",
      "Learning rate: 0.5\n",
      "Number of estimators: 400\n",
      "Accuracy (training): 0.784\n",
      "Accuracy (validation): 0.586\n",
      "Learning rate: 0.5\n",
      "Number of estimators: 500\n",
      "Accuracy (training): 0.805\n",
      "Accuracy (validation): 0.584\n",
      "Learning rate: 0.5\n",
      "Number of estimators: 600\n",
      "Accuracy (training): 0.819\n",
      "Accuracy (validation): 0.579\n",
      "Learning rate: 0.75\n",
      "Number of estimators: 100\n",
      "Accuracy (training): 0.720\n",
      "Accuracy (validation): 0.592\n",
      "Learning rate: 0.75\n",
      "Number of estimators: 200\n",
      "Accuracy (training): 0.757\n",
      "Accuracy (validation): 0.573\n",
      "Learning rate: 0.75\n",
      "Number of estimators: 300\n",
      "Accuracy (training): 0.784\n",
      "Accuracy (validation): 0.580\n",
      "Learning rate: 0.75\n",
      "Number of estimators: 400\n",
      "Accuracy (training): 0.809\n",
      "Accuracy (validation): 0.577\n",
      "Learning rate: 0.75\n",
      "Number of estimators: 500\n",
      "Accuracy (training): 0.829\n",
      "Accuracy (validation): 0.573\n",
      "Learning rate: 0.75\n",
      "Number of estimators: 600\n",
      "Accuracy (training): 0.846\n",
      "Accuracy (validation): 0.566\n",
      "Learning rate: 1\n",
      "Number of estimators: 100\n",
      "Accuracy (training): 0.717\n",
      "Accuracy (validation): 0.586\n",
      "Learning rate: 1\n",
      "Number of estimators: 200\n",
      "Accuracy (training): 0.766\n",
      "Accuracy (validation): 0.580\n",
      "Learning rate: 1\n",
      "Number of estimators: 300\n",
      "Accuracy (training): 0.797\n",
      "Accuracy (validation): 0.580\n",
      "Learning rate: 1\n",
      "Number of estimators: 400\n",
      "Accuracy (training): 0.825\n",
      "Accuracy (validation): 0.572\n",
      "Learning rate: 1\n",
      "Number of estimators: 500\n",
      "Accuracy (training): 0.843\n",
      "Accuracy (validation): 0.566\n",
      "Learning rate: 1\n",
      "Number of estimators: 600\n",
      "Accuracy (training): 0.863\n",
      "Accuracy (validation): 0.566\n",
      "The best score (validation accuracy) is: 0.6104166666666667\n",
      "Parameters (learning rate, nb of estimators): (0.1, 100)\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.1, 0.25, 0.5, 0.75, 1]\n",
    "no_estimators_list = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "bestScore = 0\n",
    "bestParams = (0,0)\n",
    "\n",
    "for lr in lr_list:\n",
    "    for n in no_estimators_list:\n",
    "        clf = GradientBoostingClassifier(n_estimators = n, learning_rate = lr)\n",
    "        clf.fit(train_X, train_Y)\n",
    "\n",
    "        print(\"Learning rate:\", lr)\n",
    "        print(\"Number of estimators:\", n)\n",
    "        print(\"Accuracy (training): {0:.3f}\".format(clf.score(train_X, train_Y)))\n",
    "        print(\"Accuracy (validation): {0:.3f}\".format(clf.score(valid_X, valid_Y)))\n",
    "\n",
    "        if clf.score(valid_X, valid_Y) > bestScore:\n",
    "            bestScore = clf.score(valid_X, valid_Y)\n",
    "            bestParams = (lr, n)\n",
    "\n",
    "print(\"The best score (validation accuracy) is:\", bestScore)\n",
    "print(\"Parameters (learning rate, nb of estimators):\", bestParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(train_X, train_Y)\n",
    "\n",
    "score = xgb_clf.score(valid_X, valid_Y)\n",
    "print(score)"
   ]
  }
 ]
}